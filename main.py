# main.py

import streamlit as st
from dotenv import load_dotenv
from workflow import criar_fluxo_agente
import os
import pandas as pd
import re

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- Configura√ß√£o da P√°gina e Estado da Sess√£o ---

st.set_page_config(page_title="Agente de An√°lise de Dados", layout="wide")

# Inicializa o estado da sess√£o se n√£o existir
def inicializar_estado_sessao():
    if "mensagens" not in st.session_state:
        st.session_state.mensagens = [{"role": "assistant", "content": "Ol√°! Configure o LLM na barra lateral e carregue um arquivo CSV ou Excel para come√ßar."}]
    if "agente_analise" not in st.session_state:
        st.session_state.agente_analise = None
    if "df" not in st.session_state:
        st.session_state.df = None

inicializar_estado_sessao()


# --- Layout da Aplica√ß√£o (Sidebar e Chat) ---

st.title("üë®‚Äçüî¨ Agente Aut√¥nomo de An√°lise de Dados")
st.write("Configure seu LLM, carregue um arquivo CSV ou Excel e fa√ßa perguntas para analis√°-lo.")

with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√£o do LLM")

    # Sele√ß√£o do provedor de LLM
    llm_provider = st.selectbox(
        "Escolha o Provedor de LLM:",
        ("Gemini", "OpenAI", "Groq", "Anthropic")
    )

    # Campo para a chave de API
    api_key = st.text_input("Insira sua Chave de API:", type="password")

    # Campo para o nome do modelo com placeholders
    model_name_placeholder = {
        "Gemini": "gemini-1.5-flash-latest",
        "OpenAI": "gpt-4-turbo",
        "Groq": "llama3-70b-8192",
        "Anthropic": "claude-3-opus-20240229"
    }.get(llm_provider, "Insira o nome do modelo")
    
    model_name = st.text_input("Nome do Modelo:", placeholder=model_name_placeholder)

    st.header("üìÇ Upload de Dados")
    uploaded_file = st.file_uploader("Escolha um arquivo CSV ou Excel", type=["csv", "xls", "xlsx"])

    # Bot√£o para criar/atualizar o agente
    if st.button("Criar/Atualizar Agente"):
        if not api_key:
            st.warning("Por favor, insira sua chave de API.")
        elif not model_name:
            st.warning(f"Por favor, insira o nome do modelo. Sugest√£o: `{model_name_placeholder}`")
        elif uploaded_file is None:
            st.warning("Por favor, carregue um arquivo CSV ou Excel.")
        else:
            with st.spinner("Processando arquivo e configurando agente..."):
                try:
                    # Carrega o DataFrame com base na extens√£o do arquivo
                    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                    if file_extension == ".csv":
                        df = pd.read_csv(uploaded_file)
                    elif file_extension in [".xls", ".xlsx"]:
                        # Para ler arquivos Excel, a biblioteca 'openpyxl' pode ser necess√°ria
                        df = pd.read_excel(uploaded_file)
                    else:
                        # Esta parte √© redundante por causa do 'type' no file_uploader, mas √© uma boa pr√°tica
                        st.error("Formato de arquivo n√£o suportado. Use CSV ou Excel.")
                        st.stop()
                    st.session_state.df = df
                    
                    # Cria o agente com as configura√ß√µes fornecidas
                    agente = criar_fluxo_agente(df, llm_provider, api_key, model_name)
                    
                    # Verifica se a cria√ß√£o do agente retornou um erro
                    if isinstance(agente, Exception):
                        st.error(f"Erro ao criar o agente: {agente}")
                        st.session_state.agente_analise = None
                    else:
                        st.session_state.agente_analise = agente
                        # Reseta o chat para a nova an√°lise
                        st.session_state.mensagens = [{"role": "assistant", "content": f"Agente configurado com `{llm_provider} ({model_name})` e arquivo `{uploaded_file.name}` carregado. O que gostaria de saber?"}]
                        st.success("Agente pronto!")
                        st.dataframe(df.head())

                except Exception as e:
                    st.error(f"Erro ao processar: {e}")
                    st.session_state.df = None
                    st.session_state.agente_analise = None

# Exibe o hist√≥rico do chat
for mensagem in st.session_state.mensagens:
    with st.chat_message(mensagem["role"]):
        st.write(mensagem["content"])

# --- L√≥gica do Chat ---

if prompt := st.chat_input("Qual sua pergunta sobre os dados?"):
    if st.session_state.agente_analise is None:
        st.warning("Por favor, carregue um arquivo CSV ou Excel na barra lateral primeiro.")
        st.stop()

    # Adiciona a mensagem do usu√°rio ao hist√≥rico e √† tela
    st.session_state.mensagens.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Invoca o agente e obt√©m a resposta
    with st.chat_message("assistant"):
        with st.spinner("Analisando..."):
            # Prepara a entrada para o agente, incluindo o hist√≥rico do chat
            entrada_agente = {
                "input": prompt,
                "chat_history": [msg for msg in st.session_state.mensagens if msg['role'] != 'assistant' or 'passos:' not in msg['content']]
            }

            try:
                # Invoca o agente
                resposta = st.session_state.agente_analise.invoke(entrada_agente)

                # Exibe os passos intermedi√°rios do agente de forma estruturada
                with st.expander("Ver Racioc√≠nio do Agente", expanded=False):
                    intermediate_steps = resposta.get("intermediate_steps", [])
                    if not intermediate_steps:
                        st.write("Nenhum passo intermedi√°rio foi executado (ex: o agente respondeu diretamente).")
                    else:
                        for i, passo in enumerate(intermediate_steps):
                            st.subheader(f"üîÑ Ciclo {i+1}")
                            acao, observacao = passo

                            # 1. Pensamento do Agente
                            st.markdown("##### 1. Pensamento")
                            # O atributo 'log' cont√©m a cadeia de pensamento completa que o LLM gerou.
                            st.text(acao.log.strip())

                            # 2. A√ß√£o Executada
                            st.markdown("##### 2. A√ß√£o")
                            st.markdown(f"**Ferramenta:** `{acao.tool}`")
                            st.markdown("**Entrada da A√ß√£o (c√≥digo executado):**")
                            st.code(acao.tool_input, language="python")

                            # 3. Observa√ß√£o (Resultado da A√ß√£o)
                            st.markdown("##### 3. Observa√ß√£o")
                            # A observa√ß√£o j√° vem formatada da nossa ferramenta customizada
                            st.markdown(observacao)
                            
                            if i < len(intermediate_steps) - 1:
                                st.divider()

                # Exibe a resposta final do agente
                resposta_final = resposta.get("output", "Desculpe, n√£o consegui obter uma resposta.")                

                # L√≥gica para lidar com o limite de itera√ß√£o atingido de forma mais amig√°vel
                if "Agent stopped due to iteration limit" in resposta_final:
                    st.warning("A an√°lise se tornou muito complexa e n√£o foi conclu√≠da no tempo limite. Aqui est√° o √∫ltimo passo do racioc√≠nio:")
                    intermediate_steps = resposta.get("intermediate_steps", [])
                    if intermediate_steps:
                        # Pega a √∫ltima observa√ß√£o, que √© o resultado mais recente que o agente viu
                        ultima_acao, ultima_observacao = intermediate_steps[-1]
                        # Exibe o √∫ltimo passo diretamente na interface
                        st.markdown("##### Pensamento")
                        st.text(ultima_acao.log.strip())
                        st.markdown("##### Observa√ß√£o")
                        st.markdown(ultima_observacao)
                    # Adiciona a mensagem de erro ao hist√≥rico
                    st.session_state.mensagens.append({"role": "assistant", "content": "A an√°lise n√£o foi conclu√≠da no tempo limite."})
                else:
                    # Se o agente concluiu, processa a resposta final normalmente
                    # L√≥gica robusta para extrair e exibir um ou mais gr√°ficos
                    chart_tag = "[CHART_PATH:"
                    if chart_tag in resposta_final:
                        parts = resposta_final.split(chart_tag)
                        if parts[0].strip():
                            st.write(parts[0])
                        for part in parts[1:]:
                            if ']' in part:
                                caminho_imagem, texto_depois = part.split(']', 1)
                                caminho_imagem = caminho_imagem.strip()
                                try:
                                    st.image(caminho_imagem, caption="Gr√°fico gerado pelo agente.", use_column_width=True)
                                except Exception as img_e:
                                    st.error(f"Erro ao exibir o gr√°fico em '{caminho_imagem}': {img_e}")
                                if texto_depois.strip():
                                    st.write(texto_depois)
                            else:
                                st.write(f"{chart_tag}{part}")
                    else:
                        st.write(resposta_final)
                    # Adiciona a resposta COMPLETA do agente ao hist√≥rico para consist√™ncia
                    st.session_state.mensagens.append({"role": "assistant", "content": resposta_final})
            except Exception as e:
                st.error("Ocorreu um erro durante a execu√ß√£o do agente. Veja os detalhes abaixo:")
                st.exception(e)
                st.session_state.mensagens.append({"role": "assistant", "content": f"Erro na execu√ß√£o: {e}"})