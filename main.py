# main.py

import streamlit as st
from dotenv import load_dotenv
from workflow import criar_fluxo_agente
import os
import pandas as pd
import re

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- Configura√ß√£o da P√°gina e Estado da Sess√£o ---

st.set_page_config(page_title="Agente de An√°lise de Dados", layout="wide")

# Inicializa o estado da sess√£o se n√£o existir
def inicializar_estado_sessao():
    if "mensagens" not in st.session_state:
        st.session_state.mensagens = []
    if "agente_analise" not in st.session_state:
        st.session_state.agente_analise = None
    if "df" not in st.session_state:
        st.session_state.df = None

inicializar_estado_sessao()


# --- Layout da Aplica√ß√£o (Sidebar e Chat) ---

st.title("üë®‚Äçüî¨ Agente Aut√¥nomo de An√°lise de Dados")

with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√£o do LLM")

    # Sele√ß√£o do provedor de LLM
    llm_provider = st.selectbox(
        "Escolha o Provedor de LLM:",
        ("LLM de Teste (Gemini)", "Gemini", "OpenAI", "Groq", "Anthropic")
    )

    api_key = ""
    model_name = ""

    # Oculta os campos de API e modelo se o LLM de teste for selecionado
    if llm_provider != "LLM de Teste (Gemini)":
        # Campo para a chave de API
        api_key = st.text_input("Insira sua Chave de API:", type="password")

        # Campo para o nome do modelo com placeholders
        model_name_placeholder = {
            "Gemini": "gemini-1.5-flash-latest",
            "OpenAI": "gpt-4-turbo",
            "Groq": "llama3-70b-8192",
            "Anthropic": "claude-3-opus-20240229"
        }.get(llm_provider, "Insira o nome do modelo")
        
        model_name = st.text_input("Nome do Modelo:", placeholder=model_name_placeholder)

    st.header("üìÇ Upload de Dados")
    uploaded_file = st.file_uploader("Escolha um arquivo CSV ou Excel", type=["csv", "xls", "xlsx"])

    # Bot√£o para criar/atualizar o agente
    if st.button("Criar/Atualizar Agente"):
        # Valida√ß√£o condicional da chave de API
        if llm_provider != "LLM de Teste (Gemini)" and not api_key:
            st.warning("Por favor, insira sua chave de API.")
        elif llm_provider != "LLM de Teste (Gemini)" and not model_name:
            st.warning(f"Por favor, insira o nome do modelo. Sugest√£o: `{model_name_placeholder}`")
        elif uploaded_file is None:
            st.warning("Por favor, carregue um arquivo CSV ou Excel.")
        else:
            with st.spinner("Processando arquivo e configurando agente..."):
                try:
                    # Carrega o DataFrame com base na extens√£o do arquivo
                    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                    if file_extension == ".csv":
                        df = pd.read_csv(uploaded_file)
                    elif file_extension in [".xls", ".xlsx"]:
                        # Para ler arquivos Excel, a biblioteca 'openpyxl' pode ser necess√°ria
                        df = pd.read_excel(uploaded_file)
                    else:
                        # Esta parte √© redundante por causa do 'type' no file_uploader, mas √© uma boa pr√°tica
                        st.error("Formato de arquivo n√£o suportado. Use CSV ou Excel.")
                        st.stop()
                    st.session_state.df = df
                    
                    # Cria o agente com as configura√ß√µes fornecidas
                    agente = criar_fluxo_agente(df, llm_provider, api_key, model_name)
                    
                    # Verifica se a cria√ß√£o do agente retornou um erro
                    if isinstance(agente, Exception):
                        st.error(f"Erro ao criar o agente: {agente}")
                        st.session_state.agente_analise = None
                    else:
                        st.session_state.agente_analise = agente
                        # Reseta o chat para a nova an√°lise
                        st.session_state.mensagens = [{"role": "assistant", "content": f"Agente configurado com `{llm_provider} ({model_name})` e arquivo `{uploaded_file.name}` carregado. O que gostaria de saber?"}]
                        st.success("Agente pronto!")
                        st.dataframe(df.head())

                except Exception as e:
                    st.error(f"Erro ao processar: {e}")
                    st.session_state.df = None
                    st.session_state.agente_analise = None

# --- Exibi√ß√£o da Interface Principal ---

# Se o agente ainda n√£o foi criado, exibe a tela de boas-vindas.
if st.session_state.agente_analise is None:
    st.info("üëã **Bem-vindo ao Agente de An√°lise de Dados!** Configure o LLM e carregue seu arquivo na barra lateral para come√ßar.")
    
    st.markdown("### üéØ Funcionalidades Principais")
    st.markdown("""
    - **An√°lise Explorat√≥ria Autom√°tica:** Descubra as caracter√≠sticas, tipos de dados e estat√≠sticas descritivas do seu dataset.
    - **Visualiza√ß√µes Inteligentes:** Gere gr√°ficos de barras, histogramas, gr√°ficos de dispers√£o e mais, de forma autom√°tica.
    - **Insights sob Demanda:** Fa√ßa perguntas em linguagem natural para extrair informa√ß√µes, resumos e realizar c√°lculos.
    - **Portabilidade de LLMs:** Escolha seu provedor preferido (Gemini, OpenAI, Groq, Anthropic) com sua pr√≥pria chave de API.
    """)

    st.markdown("### üöÄ Como Come√ßar")
    st.markdown("""
    1.  **Configure o LLM:** Na barra lateral, escolha o provedor, insira sua chave de API e o nome do modelo.
    2.  **Fa√ßa o Upload:** Carregue um arquivo CSV ou Excel (limite de 200MB).
    3.  **Crie o Agente:** Clique no bot√£o "Criar/Atualizar Agente".
    4.  **Inicie a An√°lise:** Ap√≥s a confirma√ß√£o, a interface de chat aparecer√°. Fa√ßa sua primeira pergunta!
    """)

    st.markdown("### üí° Exemplos de Perguntas")
    st.markdown("""
    - "Fa√ßa uma an√°lise explorat√≥ria inicial dos dados."
    - "Qual a quantidade de fraudes?"
    - "Gere um gr√°fico de barras mostrando a contagem de fraudes."
    """)
    st.warning("**Aviso:** O agente tem um limite de 5 intera√ß√µes por pergunta. Para an√°lises complexas, divida sua pergunta em partes menores.")

# Se o agente j√° foi criado, exibe a interface de chat.
else:
    for mensagem in st.session_state.mensagens:
        with st.chat_message(mensagem["role"]):
            st.write(mensagem["content"])

# Exibe o hist√≥rico do chat
for mensagem in st.session_state.mensagens:
    with st.chat_message(mensagem["role"]):
        st.write(mensagem["content"])

# --- L√≥gica do Chat ---

if prompt := st.chat_input("Qual sua pergunta sobre os dados?"):
    if st.session_state.agente_analise is None:
        st.warning("Por favor, carregue um arquivo CSV ou Excel na barra lateral primeiro.")
        st.stop()

    # Adiciona a mensagem do usu√°rio ao hist√≥rico e √† tela
    st.session_state.mensagens.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Invoca o agente e obt√©m a resposta
    with st.chat_message("assistant"):
        with st.spinner("Analisando..."):
            # Prepara a entrada para o agente, incluindo o hist√≥rico do chat
            # Simplificado para usar a estrutura de mensagens diretamente,
            # que j√° √© limpa e n√£o cont√©m dados de UI.
            entrada_agente = {
                "input": prompt,
                "chat_history": st.session_state.mensagens
            }

            try:
                # Invoca o agente
                resposta = st.session_state.agente_analise.invoke(entrada_agente)

                # Vari√°vel para armazenar a resposta limpa para o hist√≥rico
                resposta_limpa_para_historico = ""

                # Exibe os passos intermedi√°rios do agente de forma estruturada
                with st.expander("Ver Racioc√≠nio do Agente", expanded=False):
                    intermediate_steps = resposta.get("intermediate_steps", [])
                    if not intermediate_steps:
                        st.write("Nenhum passo intermedi√°rio foi executado (ex: o agente respondeu diretamente).")
                    else:
                        for i, passo in enumerate(intermediate_steps):
                            st.subheader(f"üîÑ Ciclo {i+1}")
                            acao, observacao = passo

                            # 1. Pensamento do Agente
                            st.markdown("##### 1. Pensamento")
                            # O atributo 'log' cont√©m a cadeia de pensamento completa que o LLM gerou.
                            st.text(acao.log.strip())

                            # 2. A√ß√£o Executada
                            st.markdown("##### 2. A√ß√£o")
                            st.markdown(f"**Ferramenta:** `{acao.tool}`")
                            st.markdown("**Entrada da A√ß√£o (c√≥digo executado):**")
                            st.code(acao.tool_input, language="python")

                            # 3. Observa√ß√£o (Resultado da A√ß√£o)
                            st.markdown("##### 3. Observa√ß√£o")
                            # A observa√ß√£o j√° vem formatada da nossa ferramenta customizada
                            st.markdown(observacao)
                            
                            if i < len(intermediate_steps) - 1:
                                st.divider()

                # Exibe a resposta final do agente
                resposta_final = resposta.get("output", "Desculpe, n√£o consegui obter uma resposta.")                

                # L√≥gica para lidar com o limite de itera√ß√£o atingido de forma mais amig√°vel
                if "Agent stopped due to iteration limit" in resposta_final:
                    st.warning("A an√°lise se tornou muito complexa e n√£o foi conclu√≠da no tempo limite. Aqui est√° o √∫ltimo passo do racioc√≠nio:")
                    intermediate_steps = resposta.get("intermediate_steps", [])
                    if intermediate_steps:
                        # Pega a √∫ltima observa√ß√£o, que √© o resultado mais recente que o agente viu
                        ultima_acao, ultima_observacao = intermediate_steps[-1]
                        # Exibe o √∫ltimo passo diretamente na interface
                        st.markdown("##### Pensamento")
                        st.text(ultima_acao.log.strip())
                        st.markdown("##### Observa√ß√£o")
                        st.markdown(ultima_observacao)
                    # Adiciona a mensagem de erro ao hist√≥rico
                    resposta_limpa_para_historico = "A an√°lise n√£o foi conclu√≠da no tempo limite."
                else:
                    # Se o agente concluiu, processa a resposta final normalmente
                    resposta_limpa_para_historico = resposta_final
                    # L√≥gica robusta para extrair e exibir um ou mais gr√°ficos
                    chart_tag = "[CHART_PATH:"
                    if chart_tag in resposta_final:
                        resposta_limpa_para_historico = re.sub(r'\[CHART_PATH:.*?\]', '', resposta_final).strip()
                        parts = resposta_final.split(chart_tag)
                        if parts[0].strip():
                            st.write(parts[0])
                        for part in parts[1:]:
                            if ']' in part:
                                caminho_imagem, texto_depois = part.split(']', 1)
                                caminho_imagem = caminho_imagem.strip()
                                try:
                                    st.image(caminho_imagem, caption="Gr√°fico gerado pelo agente.", use_column_width=True)
                                except Exception as img_e:
                                    st.error(f"Erro ao exibir o gr√°fico em '{caminho_imagem}': {img_e}")
                                if texto_depois.strip():
                                    st.write(texto_depois)
                            else:
                                st.write(f"{chart_tag}{part}")
                    else:
                        st.write(resposta_final)
                
                # Adiciona a resposta limpa (sem tags de gr√°fico) do agente ao hist√≥rico
                if resposta_limpa_para_historico:
                    st.session_state.mensagens.append({"role": "assistant", "content": resposta_limpa_para_historico})
            except Exception as e:
                st.error("Ocorreu um erro durante a execu√ß√£o do agente. Veja os detalhes abaixo:")
                st.exception(e)
                st.session_state.mensagens.append({"role": "assistant", "content": f"Erro na execu√ß√£o: {e}"})